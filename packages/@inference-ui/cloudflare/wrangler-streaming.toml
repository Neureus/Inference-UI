# Cloudflare Workers Configuration - Streaming Worker
# Dedicated worker for AI streaming responses
# Called via service binding from main API worker
# Account: FinHub

name = "inference-ui-streaming"
account_id = "d9700a3fcc05a01f5d81670ebbae817d"
main = "src-streaming/index.ts"
compatibility_date = "2024-10-01"
workers_dev = true

# Smart Placement - Automatically place worker near backend services
# Reduces latency by running worker close to Workers AI, D1, KV, and Analytics Engine
[placement]
mode = "smart"

# Workers AI - GPU-powered inference
[ai]
binding = "AI"

# KV for response caching
[[kv_namespaces]]
binding = "KV"
id = "813d5ccd33b943439c91f52b5b682372"

# D1 for usage tracking
[[d1_databases]]
binding = "DB"
database_name = "inference-ui-db"
database_id = "4de74839-04b8-4b83-8550-578701999006"

# Analytics for performance metrics
[[analytics_engine_datasets]]
binding = "ANALYTICS"

# Environment Variables
[vars]
ENVIRONMENT = "development"
API_VERSION = "v1"

# Production Environment
[env.production]
name = "inference-ui-streaming-production"
workers_dev = false

[env.production.vars]
ENVIRONMENT = "production"

# Build Configuration
[build]
command = "npm run build:streaming"

# Observability
[observability]
enabled = true
head_sampling_rate = 1
